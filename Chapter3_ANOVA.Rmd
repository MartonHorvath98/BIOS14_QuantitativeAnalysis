---
title: "Processing and Analysis of Biological Data"
subtitle: "The Linear Model II: Analysis of Variance"
author: "Ã˜ystein H. Opedal"
date: "1 Nov 2022"
output: pdf_document
fig_caption: yes
pandoc_args: ["--wrap=none"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Analysis of variance (ANOVA)
When our predictor variables are categorical (factors), linear models are used to perform analyses of variance. The parameter estimation works in much the same way as in regression, except that instead of estimating regression slopes, we are estimating group effects.

Let's simulate some data, fit a linear model, and perform an ANOVA.

```{r, fig.width=4, fig.height=4}
set.seed(100)
groups = as.factor(rep(c("Small", "Medium", "Large"), each=50))
x = c(rnorm(50, 10, 3), rnorm(50, 13, 3), rnorm(50, 14, 3))

plot(groups, x, las=1, xlab="")
```

Plots like this are called 'boxplots', and can be useful for visualising the distribution of data across factor levels or other groups. With the default settings, the boxes range from the 1st to the 3rd quartile, i.e. they span 50% of the data. The thick lines show the median, the 'whiskers' extend to 1.5 times the inter-quantile range, and individual circles show outliers. This representations allows us to assess whether the data are roughly normally distributed within each group, an assumption of the ANOVA model.

Boxplots are not directly useful for discussing ANOVA and variance partitioning though, and plots that show all the data can be more informative. Here is a rather elaborate plot combining a scatterplot (with values slightly 'jittered' along the x-axis for clarity) and a boxplot.

EXTRA EXERCISE: Reproduce a plot similar to this for the ANOVA exercise.

```{r, fig.width=4, fig.height=4, echo=F}
plot(as.numeric(groups) + rnorm(150,0,0.03), x, 
     las=1, xlab="", type="p", col="grey",
     xlim=c(0.5, 3.75), xaxt="n", yaxt="n")
axis(1, 1:3, labels=levels(groups))

means = tapply(x, groups, mean)
points(1:3, means, pch=16, col="black")

par(new=T)
plot(groups, x, at=c(1.3, 2.3, 3.3), boxwex=0.3, 
     xlim=c(0.5, 3.75), xaxt="n", yaxt="n",
     las=1, xlab="", ylab="")
```

The aim of our ANOVA analysis is to evaluate whether the variance among groups is greater than the variance within groups, or more so than expected by chance. We fit the model just as before.

```{r}
m = lm(x~groups)
anova(m)
```

The ANOVA table contains a lot of information. First, we learn about the number of degrees of freedom for each variable. For the `groups` variable (our focal factor), the 2 degrees of freedom is the number of groups in our data (3) - 1. The minus 1 comes from the fact that we had to estimate the mean in the data to obtain our sums of squares (the sum of the square deviations of data points from their group means). Similarly for residual degrees of freedom, we have 150 - 2 - 1, where the 2 comes from estimating the two contrasts (difference of group 2 and 3 from group 1), and the 1 is still the estimated mean.

The `Sum Sq` are the sums of squares, i.e. the sum of the squared deviations of each observation from the grand mean. The total sum of squared ($SS_T$) divided by $n-1$ gives the total variance of the sample.

```{r}
SS_T = 319.97+1200.43
SS_T/(150-1)
```

```{r}
var(x)
```

We can easily get the proportion of variance explained by the `groups` variable, which is the same as the $r^2$ for the model.

```{r}
319.97/SS_T
```

The `Mean Sq` is the variance attributable to each variable, conventionally called the mean sum of squares (the sum of squares divided by the degrees of freedom). The F-ratio is computed as the mean sum of squares for the focal variable divided by the mean residual sum of squares. Thus, it represents the ratio of the among-group variance to the within-group variance, but also the sample size which gives the residual degrees of freedom and thus all else being equal, larger sample gives lower mean sum of squares and thus higher F-ratios and lower $P$-values.

In an ANOVA, a statistically significant result such as the one above indicates that at least one group mean is different from the others. To further assess which groups are different, we can extract the typical summary table of the linear model.

```{r}
summary(m)
```

This contains some of the same information as the ANOVA table, but we now also obtain parameter estimates. The first parameter, the intercept, corresponds to the estimated mean for the first level of the `groups` factor. In this example this happens to be 'Large', because L comes before M and S in the alphabet. The next two estimates represents *contrasts* from the reference group, and the associated hypothesis tests tests the null hypothesis that the group has the same mean as the reference group.

The summary table also gives us directly the $r^2$, which is a simple ANOVA is defined as $1-SS_E/SS_T$, where $SS_E$ is the sum of squares for the error term (residuals), and $SS_T$ is the total sum of squared. (Control question: why could we do it even more simply above?).

The parameter estimates allow us to quantify the effect size, i.e. the magnitude of the difference between the groups. A useful way to report such differences is to compute the % difference (the contrast divided by the mean of the reference group, here 3.277/13.4642 = 0.243), so that we can say that 'Small individuals were 24.3% smaller than large individuals'.

Note that if we want a different reference group, we can change the order of the factor levels.

```{r}
groups = factor(groups, levels=c("Small", "Medium", "Large"))
m = lm(x~groups)
summary(m)
```

Sometimes we also want to suppress the intercept of the model, and thus estimate the mean and standard error for each level of the predictor. We can do this by adding `-1` to the model formula (what comes after the `~` sign). This could be useful for example, if we wanted to obtain the estimated mean for each group, associated for example with a 95% confidence interval.

```{r}
m = lm(x~groups-1)
summary(m)$coef
confint(m)
```

Tukey

## The linear model III: two-way ANOVA
Analyses of variance can also be performed with more than one factor variable. If we have two factors, we can talk about two-way ANOVA, and so on. A typical example from biology is when we have performed a factorial experiment, and want to assess the effects of each experimental factor and their potential interaction.

With two factors, a full model can be formulated as `y ~ factor1 * factor2`. Recall that in `R`syntax, the * means both main effects and their interaction, while a : means only the interaction. A detectable interaction term in this model would indicate that the effect of factor 1 depends on the level of factor 2 (and *vice versa*). If we are analysing an experiment where we have manipulated both temperature and nitrogen supply, an interaction would mean that the effect of temperature depend on the nitrogen level.

### Data exercise: analysing a factorial experiment

```{r}
dat = read.csv("datasets/butterflies.csv")
names(dat)
```


### Data exercise: Interpreting linear-model analyses

Flowers are integrated phenotypes, which means that the different parts of the flowers are generally covarying with each other so that large flowers have e.g. both longer petals and longer sepals. Evolutionary botanists are interested in these patterns of covariation among floral parts, because they can affect for example the fit of flowers to their pollinators. We will work with a dataset on flower measurements from 9 natural populations in Costa Rica.

The traits are

- ASD: anther-stigma distance ($mm$)
- GAD: gland-anther distance ($mm$)
- GSD: gland-stigma distance ($mm$)
- LBL: lower bract length ($mm$)
- LBW: lower bract width ($mm$)
- UBL: upper bract length ($mm$)
- UBW: upper bract width ($mm$)
- GW: gland width ($mm$)
- GA: gland area ($mm^2$)

The traits have known or assumed functions. Anther-stigma distance is important for the ability of self-pollination, gland-anther distance and gland-stigmas distance affect the fit of flowers to pollinators, the upper and lower bracts are advertisements (think petals in other flowers), and the gland produces the the reward for pollinators.

The first step in any data analysis in always to explore the data. Make a series of histograms and plots. How are the data distributed? Are there any problematic outliers? How are patterns of trait correlations? Which traits are (proportionally) more variable?

What about differences between populations? Are any of the traits detectably different? By 'detectably' I mean statisticaly significant, but because the 's word' is so often misused, I find it safer to say detectably. At the very least, say 'statistically significant', to make clear that you do not (necessarily) imply any biological significance.

To get started, the following lines reads the data.

```{r}
blossoms = read.csv("datasets/blossoms/blossoms.csv")
names(blossoms)
```

To summarize the data per population, the `apply` family of functions are useful. To call a function for each level of a factor, such as computing the mean for each population, we can use `tapply`.

```{r}
tapply(blossoms$UBW, blossoms$pop, mean, na.rm=T)
```

A couple of packages are also very useful for producing complete summaries. I use `plyr` and `reshape2`.  You could also consider learning some of the more modern things such as `tidyverse`.
 
```{r}
library(plyr)
library(knitr)
popstats = ddply(blossoms, .(pop), summarize,
                 LBWm = mean(LBW, na.rm=T),
                 LBWsd = sd(LBW, na.rm=T),
                 GSDm = mean(GSD, na.rm=T),
                 GSDsd = sd(GSD, na.rm=T),
                 ASDm = mean(ASD, na.rm=T),
                 ASDsd = sd(ASD, na.rm=T))
popstats[,-1] = round(popstats[,-1], 2)
kable(popstats)
```

After exploring and summarizing the data, fit some linear models to estimate the slopes of one trait on another. Interpret the results. Do the analysis on both arithmetic and log scale. Choose traits that belong to the same vs. different functional groups, can you detect any patterns? Produce tidy figures that illustrate the results. Hint: once you have produced a scatterplot, you can add more points (e.g. for a different variable) by using the `points()` function.

```{r, fig.height=4, fig.width=4, include=F, echo=F}
plot(log(blossoms$LBW), log(blossoms$UBW), ylim=c(1,3.5), las=1,
     xlab="Lower bract width( log mm)",
     ylab="UBW/GSD (log mm)")
points(log(blossoms$LBW), log(blossoms$GSD), pch=16)
```

```{r, include=F, echo=F}
mUBW = lm(log(UBW)~pop, data=blossoms)
mGSD = lm(log(GSD)~pop, data=blossoms)
anova(mUBW)
anova(mGSD)
```

```{r, include=F, echo=F}
m = lm(log(GSD)~log(LBW)*pop, data=blossoms)
anova(m)
```


---
title: "Processing and Analysis of Biological Data"
subtitle: "The Linear Model II: Analysis of Variance"
author: "Ã˜ystein H. Opedal"
date: "8 Nov 2022"
output: pdf_document
fig_caption: yes
pandoc_args: ["--wrap=none"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Analysis of variance (ANOVA)
When our predictor variables are categorical (factors), linear models are used to perform analyses of variance. The parameter estimation works in much the same way as in regression, except that instead of estimating regression slopes, we are estimating group effects.

Let's simulate some data, fit a linear model, and perform an ANOVA.

```{r, fig.width=4, fig.height=4}
set.seed(100)
groups = as.factor(rep(c("Small", "Medium", "Large"), each=50))
x = c(rnorm(50, 10, 3), rnorm(50, 13, 3), rnorm(50, 14, 3))

plot(groups, x, las=1, xlab="")
```

Plots like this are called 'boxplots', and can be useful for visualising the distribution of data across factor levels or other groups. With the default settings, the boxes range from the 1st to the 3rd quartile, i.e. they span 50% of the data. The thick lines show the median, the 'whiskers' extend to 1.5 times the inter-quantile range, and individual circles show outliers. This representations allows us to assess whether the data are roughly normally distributed within each group, an assumption of the ANOVA model.

Boxplots are not directly useful for discussing ANOVA and variance partitioning though, and plots that show all the data can be more informative. Here is a rather elaborate plot combining a scatterplot (with values slightly 'jittered' along the x-axis for clarity) and a boxplot.

EXTRA EXERCISE: Reproduce a plot similar to this for the ANOVA exercise.

```{r, fig.width=4, fig.height=4, echo=F}
plot(as.numeric(groups) + rnorm(150,0,0.03), x, 
     las=1, xlab="", type="p", col="grey",
     xlim=c(0.5, 3.75), xaxt="n", yaxt="n")
axis(1, 1:3, labels=levels(groups))

means = tapply(x, groups, mean)
points(1:3, means, pch=16, col="black")

par(new=T)
plot(groups, x, at=c(1.3, 2.3, 3.3), boxwex=0.3, 
     xlim=c(0.5, 3.75), xaxt="n", yaxt="n",
     las=1, xlab="", ylab="")
```

The aim of our ANOVA analysis is to evaluate whether the variance among groups is greater than the variance within groups, or more so than expected by chance. We fit the model just as before.

```{r}
m = lm(x~groups)
anova(m)
```

The ANOVA table contains a lot of information. First, we learn about the number of degrees of freedom for each variable. For the `groups` variable (our focal factor), the 2 degrees of freedom is the number of groups in our data (3) - 1. The minus 1 comes from the fact that we had to estimate the mean in the data to obtain our sums of squares (the sum of the square deviations of data points from their group means). Similarly for residual degrees of freedom, we have 150 - 2 - 1, where the 2 comes from estimating the two contrasts (difference of group 2 and 3 from group 1), and the 1 is still the estimated mean.

The `Sum Sq` are the sums of squares, i.e. the sum of the squared deviations of each observation from the grand mean. The total sum of squared ($SS_T$) divided by $n-1$ gives the total variance of the sample.

```{r}
SS_T = 319.97+1200.43
SS_T/(150-1)
```

```{r}
var(x)
```

We can easily get the proportion of variance explained by the `groups` variable, which is the same as the $r^2$ for the model.

```{r}
319.97/SS_T
```

The `Mean Sq` is the variance attributable to each variable, conventionally called the mean sum of squares (the sum of squares divided by the degrees of freedom). The F-ratio is computed as the mean sum of squares for the focal variable divided by the mean residual sum of squares. Thus, it represents the ratio of the among-group variance to the within-group variance, but also the sample size which gives the residual degrees of freedom and thus all else being equal, larger sample gives lower mean sum of squares and thus higher F-ratios and lower $P$-values.

In an ANOVA, a statistically significant result such as the one above indicates that at least one group mean is different from the others. To further assess which groups are different, we can extract the typical summary table of the linear model.

```{r}
summary(m)
```

This contains some of the same information as the ANOVA table, but we now also obtain parameter estimates. The first parameter, the intercept, corresponds to the estimated mean for the first level of the `groups` factor. In this example this happens to be 'Large', because L comes before M and S in the alphabet. The next two estimates represents *contrasts* from the reference group, and the associated hypothesis tests tests the null hypothesis that the group has the same mean as the reference group.

The summary table also gives us directly the $r^2$, which is a simple ANOVA is defined as $1-SS_E/SS_T$, where $SS_E$ is the sum of squares for the error term (residuals), and $SS_T$ is the total sum of squared. (Control question: why could we do it even more simply above?).

The parameter estimates allow us to quantify the effect size, i.e. the magnitude of the difference between the groups. A useful way to report such differences is to compute the % difference (the contrast divided by the mean of the reference group, here 3.277/13.4642 = 0.243), so that we can say that 'Small individuals were 24.3% smaller than large individuals'.

Note that if we want a different reference group, we can change the order of the factor levels.

```{r}
groups = factor(groups, levels=c("Small", "Medium", "Large"))
m = lm(x~groups)
summary(m)
```

Sometimes we also want to suppress the intercept of the model, and thus estimate the mean and standard error for each level of the predictor. We can do this by adding `-1` to the model formula (what comes after the `~` sign). This could be useful for example, if we wanted to obtain the estimated mean for each group, associated for example with a 95% confidence interval.

```{r}
m = lm(x~groups-1)
summary(m)$coef
confint(m)
```

Tukey

## The linear model III: two-way ANOVA
Analyses of variance can also be performed with more than one factor variable. If we have two factors, we can talk about two-way ANOVA, and so on. A typical example from biology is when we have performed a factorial experiment, and want to assess the effects of each experimental factor and their potential interaction.

With two factors, a full model can be formulated as `y ~ factor1 * factor2`. Recall that in `R`syntax, the * means both main effects and their interaction, while a : means only the interaction. A detectable interaction term in this model would indicate that the effect of factor 1 depends on the level of factor 2 (and *vice versa*). If we are analysing an experiment where we have manipulated both temperature and nitrogen supply, an interaction would mean that the effect of temperature depend on the nitrogen level.

### Data exercise: analysing a factorial experiment
The following data are from an experiment where female butterflies reared on two different host plants were allowed to oviposit on the same two host plants. The data include developmental time for the larvae, the adult weight, and the growth rate of the larvae.

Analyse the data to assess effects of maternal vs. larval host plant on one or more response variable. Interpret the results and produce a nice plot to illustrate.


```{r}
dat = read.csv("datasets/butterflies.csv")
names(dat)
```
```{r, fig.height=4, fig.width=4, echo=T}
dat$MaternalHost = paste0(dat$MaternalHost, "M")
dat$LarvalHost = paste0(dat$LarvalHost, "L")

means = tapply(dat$DevelopmentTime, list(dat$MaternalHost, dat$LarvalHost), mean)
ses = tapply(dat$DevelopmentTime, 
       list(dat$MaternalHost, dat$LarvalHost), 
       function(x) sd(x)/sqrt(sum(!is.na(x))))

means
ses

plot(c(0.97, 1.03), means[1,], ylim=c(18, 40), xlim=c(0.8, 2.2),
     xlab="Larval host", 
     ylab="Developmental time (days)",
     xaxt="n", las=1, pch=c(21,16), col="white")
axis(1, 1:2, labels=c("Barbarea", "Berteroa"))

arrows(c(0.97,1.03), means[1,]-ses[1,], c(0.97,1.03), 
       means[1,]+ses[1,], length=0.05, angle=90, code=3)
arrows(c(1.97,2.03), means[2,]-ses[2,], c(1.97,2.03), 
       means[2,]+ses[2,], length=0.05, angle=90, code=3)

segments(0.97, means[1,1], 1.97, means[2,1], lty=2)
segments(1.03, means[1,2], 2.03, means[2,2])

points(c(0.97, 1.03), means[1,], pch=c(21,16), bg="white")
points(c(1.97, 2.03), means[2,], pch=c(21, 16), bg="white")

legend("topleft", c("Maternal host", "Barbarea", "Berteroa"), 
       bty="n", pch=c(NA,21,16))
```

```{r}
names(dat)
m = lm(DevelopmentTime~MaternalHost*LarvalHost, data=dat)
anova(m)
summary(m)
```



---
title: "Measuring natural selection"
author: "Ã˜ystein H. Opedal"
date: "9 May 2022"
output: pdf_document
fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```

## Introduction

Natural selection may occur whenever individuals in a population differ in fitness. If these individuals differ also in phenotypic traits, so that certain trait values are associated with variation in fitness, we say that phenotypic selection acts on those traits. If those traits are also heritable (i.e. they exhibit some additive genetic variation), natural selection may also lead to evolution (i.e. response to selection).

The simple Breeder's equation illustrates the two-step process by which natural selection may lead to evolution:

$R = h^2S$,

where $h^2 = V_A/V_P$ is the narrow-sense heritability, and $S$ is the selection differential $S = cov(w,z)$, where $w$ is relative fitness defined as $w = W/\bar{W}$.

In some cases, such as in artificial-selection studies, we can directly compute the selection differential as the difference between the trait mean before selection and the trait mean after selection. To measure selection in natural populations, however, this is often not practical.

Instead, to study selection on phenotypic traits in natural populations, we can use regression techniques to relate variation in the fitness of individuals to variation in their phenotypic traits. To see why this makes sense, we can write the Breeder's equation as

$R = \frac{V_A}{V_P}cov(w,z) = V_A \frac{cov(w,z)}{V_P}$.

Recall that $\beta=cov(w,z)/V_P$ is the regression slope of $w$ on $z$. This formulation shows that we can estimate the strength of selection in natural populations by regressing relative fitness on trait values, an important result first discussed by Pearson (1903), and extended and operationalized in 1983 in a landmark paper by Russ Lande and Stevan J. Arnold.

## The Lande-Arnold approach to measuring selection

Lande & Arnold (1983) operationalized the use of multiple regression to measure phenotypic selection in natural populations. Linear selection gradients, conventionally denoted as $\beta$, describe the local slope of the fitness surface describing the relationship between traits and fitness. We can estimate $\beta$ as the partial regression coefficients of relative fitness ($w$) on a trait vector $z$. We can write this linear model as

$y_i = \alpha + \sum_{j} x_i\beta_j + \epsilon_i$

where $y_i$ is the relative fitness of individual $i$, $\alpha$ is an intercept, and the $\beta$'s are partial regression slopes.

To make selection gradients comparable across traits and species, they are normally standardized either by the phenotypic standard deviation $\beta_\sigma = \beta\,\sigma(x)$, or by the trait mean $\beta_\mu = \beta\,\mu(x)$.

The variance-standardized selection gradient is also referred to as the selection intensity $i = \frac{S}{\sigma(x)}$, and measures the proportional change in fitness per standard deviation change in the trait. 

The mean-standardized selection gradient measure the proportional change in fitness per proportional change in the trait, and is technically an elasticity. Note that if the trait $z$ is relative fitness, then the mean-standardized selection gradient is 1 (because we regress relative fitness on itself). This provides a useful benchmark for judging the strength of selection.

## Exercise 1: Selection on *Ipomopsis aggragata* floral traits

To demonstrate the Lande-Arnold regression approach, we will analyse data collected by Campbell & Powers (2015) on floral traits and seed set of the plant *Ipomopsis aggregata*.

### Read datafile

```{r}
dat = read.table("campbellpowers.txt", header=T)
names(dat)
```

### Explore data

Before enbarking on any statistical analysis, it is advisable to explore the data graphically.

```{r, fig.height=4, fig.width=10}
par(mfrow=c(1,3))
plot(dat$Corollalength, dat$Seeds)
plot(dat$Corollawidth, dat$Seeds)
plot(dat$Flowers, dat$Seeds)
```

\pagebreak

### Estimate variance-standardized univariate selection gradients

Univariate selection gradients measure net selection on the focal trait, and can be estimated as a simple regression of relative fitness on the trait. To obtain variance-standardized selection gradients, we scale the trait to zero mean and unit variance using the `scale` function in R.

```{r}
# Define relative fitness
dat$relfit = dat$Seeds/mean(dat$Seeds, na.rm=T)

summary(lm(relfit ~ scale(Flowers), na=na.exclude, data=dat))
```

The univariate directonal selection gradient on flower number is 0.96, which means that fitness nearly doubles (or changes by approximately 96%) per standard deviation increase in flower number.

Similarly, we can estimate the univariate selection gradients for corolla length and width:

```{r}
summary(lm(relfit ~ scale(Corollalength), na=na.exclude, data=dat))$coef
summary(lm(relfit ~ scale(Corollawidth), na=na.exclude, data=dat))$coef
```

### The opportunity for selection

A requirement for natural selection is that there is variation in fitness among individuals. We can measure the *opportunity for selection* as the variance in relative fitness, $I = var(\frac{W}{\bar{W}})$. Conveniently, the maximum value of the variance-standardized selection gradient is the square root of $I$, $i = \sqrt{I}$.

```{r}
I = var(dat$relfit)
signif(sqrt(I), 3)
```

This suggests that selection on flower number is strong, as the variance-standardized selection gradient is $0.96/\sqrt(I)=$ `r signif(0.95587/sqrt(I),2)*100`% of its maximum value.

### Estimate mean-standardized univariate selection gradients

As noted above, variance-standardization is not the only way to make selection gradients comparable across traits and studies. Let us estimate the mean-standardized selection gradients. We can do this either by dividing the trait values by their mean prior to fitting the model, or by multiplying the raw selection gradient by the trait mean as we do below.

```{r}
summary(lm(relfit ~ scale(Flowers, scale=F), na=na.exclude, data=dat))$coef
0.0167463*mean(dat$Flowers, na.rm=T)

summary(lm(relfit ~ scale(Corollalength, scale=F), na=na.exclude, data=dat))$coef
0.06004*mean(dat$Corollalength, na.rm=T)

summary(lm(relfit ~ scale(Corollawidth, scale=F), na=na.exclude, data=dat))$coef
0.04810*mean(dat$Corollawidth, na.rm=T)
```

Selection on flower number and corolla length is stronger than selection on fitness as a trait($\beta_\mu = 1$), which is very strong.

### Estimate variance-standardized multivariate selection gradients

Univariate selection gradients measure the net strength of selection on a trait, including direct selection on the trait, and indirect selection due to selection on trait that are phenotypically correlated with the focal trait. In a multiple-regression model, the coefficient for each independent variable is estimated while holding the other independent variables constant at their mean. The key advantage of the multiple-regression approach of Lande and Arnold is therefore that selection gradients estimated as partial regression coefficients accounts for indirect selection. 

```{r}
m = lm(relfit~scale(Flowers) + scale(Corollalength) + scale(Corollawidth), na=na.exclude, data=dat)
summary(m)$coef
```

We notice that selection on flower number is still strong, while the selection gradients on corolla length and width have changed. To see how the multiple-regression approach separates direct from indirect selection, we can compute the univariate selection gradient (referred to here as *s* for simplicity) from the coefficients above. For corolla length (trait 2), for example,this is given by

$s=\beta_1r_{12}+\beta_2+\beta_3r_{23}$, where $r_{12}$ is the phenotypic correlation between trait 1 and trait 2. 

```{r}
beta_flowers = summary(m)$coef[2,1]
beta_length = summary(m)$coef[3,1]
beta_width = summary(m)$coef[4,1]

beta_length + 
  beta_flowers*cor(dat$Corollalength, dat$Flowers, use="pairwise") +
  beta_width*cor(dat$Corollalength, dat$Corollawidth, use="pairwise")
```

This demonstrates that the net selection gradient on a trait includes the indirect selection due to phenotypic correlations with another trait under selection (e.g. flower number in the case of corolla length). For all the traits, we can do this with matrix algebra, computing

$s^T=\beta\mathbf{P}$, where $\mathbf{P}$ is the phenotypic correlation matrix. Matrix multiplication in R is done using the `%*%` operator.

```{r}
beta = summary(m)$coef[2:4,1]
signif(t(beta), 2)

cormat = cor(dat[,3:5], use="pairwise")[c(3,1,2),c(3,1,2)]
signif(cormat, 2)

beta%*%cormat
```

## Exercise 2: Selection on *Dalechampia scandens* blossom traits

### Read datafile

```{r}
dal = read.csv("Dalechampia.csv")
head(dal)
```

### Explore data

```{r, fig.height=4, fig.width=4}
dal$relfit = dal$total_seeds/mean(dal$total_seeds, na.rm=T)
dal$pollentot = dal$pollenfem + dal$pollenmale

signif(cor(dal[,c(2,3,5,6)], use="pairwise"), 2)
plot(dal$UBA, dal$pollenfem)
```

### Univariate selection gradients for Upper Bract Area

```{r}
m = lm(relfit ~ scale(UBA, scale=F), na=na.exclude, data=dal)
summary(m)

summary(m)$coef[2,1]*sd(dal$UBA, na.rm=T)
summary(m)$coef[2,1]*mean(dal$UBA, na.rm=T)
```

### Multivariate selection gradients

```{r}
m = lm(relfit ~ scale(UBA) + scale(GA) + scale(GSD) + scale(ASD), na=na.exclude, data=dal)
summary(m)$coef
```

### Selection through pollen arrival

To obtain the most accurate measure of selection, we would like our fitness measure to be as 'close' as possible to fitness. In some cases, such as when working with annual plants, the number of seeds produced may be a good proxy of fitness, but in most cases we are stuck with some fitness component such as the seed set in a given year in a perennial plant.

In the conetext of pollinator-mediated selection on floral traits, we are interested in how the interactions between flowers and pollinators affect fitness, thus learning about the functional components of selection, that we expect over time to lead to selection and adaptation.

The number of seeds produced may depend on many factors, including the number of pollen grains arriving onto stigmas, and the environment of the mother plant. Selection gradients estimated using seed set as a fitness component may therefore not represent purely pollinator-mediated selection. In these cases, much can be learned by considering lower-level fitness components such as the number of pollen grains deposited onto stigmas by pollinators.

*Dalechampia* blossoms are functionally protogynous, which means that during the development of the blossom there is a female phase of several days before the first male flower opens. All pollen deposited during this phase is necessarily deposited by pollinators.  

Because the number of pollen grains is a count variable, we fit a generalized linear model with Poisson errors. Conveniently, because of the log link function, regression coefficients from a Poisson GLM are roughly identical to selection gradients estimated from a linear model with relative fitness as the response. This is because the variance of a natural log-transformed variable is almost identical to that of a mean-scaled variable.

\pagebreak

```{r}
m = glm(pollenfem ~ scale(UBA, scale=F), family="poisson", na=na.exclude, data=dal)
summary(m)$coef
summary(m)$coef[2,1]*sd(dal$UBA, na.rm=T)
summary(m)$coef[2,1]*mean(dal$UBA, na.rm=T)
```

